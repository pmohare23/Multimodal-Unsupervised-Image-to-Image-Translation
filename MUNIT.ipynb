{"cells":[{"cell_type":"code","source":["#https://drive.google.com/file/d/1oJ7-hV3smTG5V7lp6EU3Ilvyso0QmWV_/view?usp=sharing\n"],"metadata":{"id":"C9WqX7KKIP3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"t36cithOKItU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670695188764,"user_tz":300,"elapsed":236,"user":{"displayName":"Pratik Mohare","userId":"13234718444931704766"}},"outputId":"5aea0fc2-838a-4626-9462-ec799f36501c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(b1=0.5, b2=0.999, batch_size=1, channels=3, checkpoint_interval=-1, dataset_name='summer2winter_yosemite', decay_epoch=100, dim=64, epoch=0, img_height=128, img_width=128, lr=0.0001, n_cpu=8, n_downsample=2, n_epochs=200, n_residual=3, sample_interval=400, style_dim=8)\n"]}],"source":["import argparse\n","import os\n","import numpy as np\n","import math\n","import itertools\n","import datetime\n","import time\n","import sys\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","from models import *\n","from datasets import *\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n","parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n","parser.add_argument(\"--dataset_name\", type=str, default=\"edges2shoes\", help=\"name of the dataset\")\n","parser.add_argument(\"--batch_size\", type=int, default=1, help=\"size of the batches\")\n","parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"adam: learning rate\")\n","parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n","parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n","parser.add_argument(\"--img_height\", type=int, default=128, help=\"size of image height\")\n","parser.add_argument(\"--img_width\", type=int, default=128, help=\"size of image width\")\n","parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n","parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval saving generator samples\")\n","parser.add_argument(\"--checkpoint_interval\", type=int, default=-1, help=\"interval between saving model checkpoints\")\n","parser.add_argument(\"--n_downsample\", type=int, default=2, help=\"number downsampling layers in encoder\")\n","parser.add_argument(\"--n_residual\", type=int, default=3, help=\"number of residual blocks in encoder / decoder\")\n","parser.add_argument(\"--dim\", type=int, default=64, help=\"number of filters in first encoder layer\")\n","parser.add_argument(\"--style_dim\", type=int, default=8, help=\"dimensionality of the style code\")\n","opt = parser.parse_args([])\n","print(opt)\n","\n","cuda = torch.cuda.is_available()\n","\n","# Create sample and checkpoint directories\n","os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n","os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9u_LNKIRXxh"},"outputs":[],"source":["f=opt.dataset_name+\".tar.gz\"\n","url=\"http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/\"+f\n","!wget $url\n","!tar -xvzf $f"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"Z_GdEjfHSpZw","outputId":"6ea78454-9d2a-4b7d-a856-85d76777f6c5","executionInfo":{"status":"error","timestamp":1670695226631,"user_tz":300,"elapsed":417,"user":{"displayName":"Pratik Mohare","userId":"13234718444931704766"}}},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-cec54e0a97f1>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    transforms.Resize((opt.img_height, opt.img_width), '''Image.BICUBIC),''' transforms.InterpolationMode.BICUBIC)\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["criterion_recon = torch.nn.L1Loss()\n","\n","# Initialize encoders, generators and discriminators\n","Enc1 = Encoder(dim=opt.dim, n_downsample=opt.n_downsample, n_residual=opt.n_residual, style_dim=opt.style_dim)\n","Dec1 = Decoder(dim=opt.dim, n_upsample=opt.n_downsample, n_residual=opt.n_residual, style_dim=opt.style_dim)\n","Enc2 = Encoder(dim=opt.dim, n_downsample=opt.n_downsample, n_residual=opt.n_residual, style_dim=opt.style_dim)\n","Dec2 = Decoder(dim=opt.dim, n_upsample=opt.n_downsample, n_residual=opt.n_residual, style_dim=opt.style_dim)\n","D1 = MultiDiscriminator()\n","D2 = MultiDiscriminator()\n","\n","if cuda:\n","    Enc1 = Enc1.cuda()\n","    Dec1 = Dec1.cuda()\n","    Enc2 = Enc2.cuda()\n","    Dec2 = Dec2.cuda()\n","    D1 = D1.cuda()\n","    D2 = D2.cuda()\n","    criterion_recon.cuda()\n","\n","if opt.epoch != 0:\n","    # Load pretrained models\n","    Enc1.load_state_dict(torch.load(\"saved_models/%s/Enc1_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","    Dec1.load_state_dict(torch.load(\"saved_models/%s/Dec1_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","    Enc2.load_state_dict(torch.load(\"saved_models/%s/Enc2_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","    Dec2.load_state_dict(torch.load(\"saved_models/%s/Dec2_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","    D1.load_state_dict(torch.load(\"saved_models/%s/D1_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","    D2.load_state_dict(torch.load(\"saved_models/%s/D2_%d.pth\" % (opt.dataset_name, opt.epoch)))\n","else:\n","    # Initialize weights\n","    Enc1.apply(weights_init_normal)\n","    Dec1.apply(weights_init_normal)\n","    Enc2.apply(weights_init_normal)\n","    Dec2.apply(weights_init_normal)\n","    D1.apply(weights_init_normal)\n","    D2.apply(weights_init_normal)\n","\n","# Loss weights\n","lambda_gan = 1\n","lambda_id = 10\n","lambda_style = 1\n","lambda_cont = 1\n","lambda_cyc = 0\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(\n","    itertools.chain(Enc1.parameters(), Dec1.parameters(), Enc2.parameters(), Dec2.parameters()),\n","    lr=opt.lr,\n","    betas=(opt.b1, opt.b2),\n",")\n","optimizer_D1 = torch.optim.Adam(D1.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n","optimizer_D2 = torch.optim.Adam(D2.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n","\n","# Learning rate update schedulers\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",")\n","lr_scheduler_D1 = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_D1, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",")\n","lr_scheduler_D2 = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_D2, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",")\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","\n","# Configure dataloaders\n","transforms_ = [\n","    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","dataloader = DataLoader(\n","    ImageDataset(opt.dataset_name, transforms_=transforms_),\n","    batch_size=opt.batch_size,\n","    shuffle=True,\n","    num_workers=opt.n_cpu,\n",")\n","\n","val_dataloader = DataLoader(\n","    ImageDataset(opt.dataset_name, transforms_=transforms_, mode=\"val\"),\n","    batch_size=5,\n","    shuffle=True,\n","    num_workers=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUE9zNNCe8no"},"outputs":[],"source":["def sample_images(batches_done):\n","    \"\"\"Saves a generated sample from the validation set\"\"\"\n","    imgs = next(iter(val_dataloader))\n","    img_samples = None\n","    for img1, img2 in zip(imgs[\"A\"], imgs[\"B\"]):\n","        # Create copies of image\n","        X1 = img1.unsqueeze(0).repeat(opt.style_dim, 1, 1, 1)\n","        X1 = Variable(X1.type(Tensor))\n","        # Get random style codes\n","        s_code = np.random.uniform(-1, 1, (opt.style_dim, opt.style_dim))\n","        s_code = Variable(Tensor(s_code))\n","        # Generate samples\n","        c_code_1, _ = Enc1(X1)\n","        X12 = Dec2(c_code_1, s_code)\n","        # Concatenate samples horizontally\n","        X12 = torch.cat([x for x in X12.data.cpu()], -1)\n","        img_sample = torch.cat((img1, X12), -1).unsqueeze(0)\n","        # Concatenate with previous samples vertically\n","        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n","    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yXof5WwKZOx"},"outputs":[],"source":["# ----------\n","#  Training\n","# ----------\n","lG1=[]\n","lG2=[]\n","lI1=[]\n","lI2=[]\n","lS1=[]\n","lS2=[]\n","lC1=[]\n","lC2=[]\n","lCy1=[]\n","lCy2=[]\n","totL=[]\n","# Adversarial ground truths\n","valid = 1\n","fake = 0\n","\n","prev_time = time.time()\n","for epoch in range(opt.epoch, opt.n_epochs):\n","    lG1.append(0)\n","    lG2.append(0)\n","    lI1.append(0)\n","    lI2.append(0)\n","    lS1.append(0)\n","    lS2.append(0)\n","    lC1.append(0)\n","    lC2.append(0)\n","    lCy1.append(0)\n","    lCy2.append(0)\n","    totL.append(0)\n","    for i, batch in enumerate(dataloader):\n","\n","        # Set model input\n","        X1 = Variable(batch[\"A\"].type(Tensor))\n","        X2 = Variable(batch[\"B\"].type(Tensor))\n","\n","        # Sampled style codes\n","        style_1 = Variable(torch.randn(X1.size(0), opt.style_dim, 1, 1).type(Tensor))\n","        style_2 = Variable(torch.randn(X1.size(0), opt.style_dim, 1, 1).type(Tensor))\n","\n","        # -------------------------------\n","        #  Train Encoders and Generators\n","        # -------------------------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Get shared latent representation\n","        c_code_1, s_code_1 = Enc1(X1)\n","        c_code_2, s_code_2 = Enc2(X2)\n","\n","        # Reconstruct images\n","        X11 = Dec1(c_code_1, s_code_1)\n","        X22 = Dec2(c_code_2, s_code_2)\n","\n","        # Translate images\n","        X21 = Dec1(c_code_2, style_1)\n","        X12 = Dec2(c_code_1, style_2)\n","\n","        # Cycle translation\n","        c_code_21, s_code_21 = Enc1(X21)\n","        c_code_12, s_code_12 = Enc2(X12)\n","        X121 = Dec1(c_code_12, s_code_1) if lambda_cyc > 0 else 0\n","        X212 = Dec2(c_code_21, s_code_2) if lambda_cyc > 0 else 0\n","\n","        # Losses\n","        loss_GAN_1 = lambda_gan * D1.compute_loss(X21, valid)\n","        loss_GAN_2 = lambda_gan * D2.compute_loss(X12, valid)\n","        loss_ID_1 = lambda_id * criterion_recon(X11, X1)\n","        loss_ID_2 = lambda_id * criterion_recon(X22, X2)\n","        loss_s_1 = lambda_style * criterion_recon(s_code_21, style_1)\n","        loss_s_2 = lambda_style * criterion_recon(s_code_12, style_2)\n","        loss_c_1 = lambda_cont * criterion_recon(c_code_12, c_code_1.detach())\n","        loss_c_2 = lambda_cont * criterion_recon(c_code_21, c_code_2.detach())\n","        loss_cyc_1 = lambda_cyc * criterion_recon(X121, X1) if lambda_cyc > 0 else 0\n","        loss_cyc_2 = lambda_cyc * criterion_recon(X212, X2) if lambda_cyc > 0 else 0\n","\n","        # Total loss\n","        loss_G = (\n","            loss_GAN_1\n","            + loss_GAN_2\n","            + loss_ID_1\n","            + loss_ID_2\n","            + loss_s_1\n","            + loss_s_2\n","            + loss_c_1\n","            + loss_c_2\n","            + loss_cyc_1\n","            + loss_cyc_2\n","        )\n","\n","        lG1[-1]+=loss_GAN_1.item()\n","        lG2[-1]+=loss_GAN_2.item()\n","        lI1[-1]+=loss_ID_1.item()\n","        lI2[-1]+=loss_ID_2.item()\n","        lS1[-1]+=loss_s_1.item()\n","        lS2[-1]+=loss_s_2.item()\n","        lC1[-1]+=loss_c_1.item()\n","        lC2[-1]+=loss_c_2.item()\n","        lCy1[-1]+=loss_cyc_1\n","        lCy2[-1]+=loss_cyc_2\n","        totL[-1]+=loss_G.item()\n","\n","        loss_G.backward()\n","        optimizer_G.step()\n","\n","        # -----------------------\n","        #  Train Discriminator 1\n","        # -----------------------\n","\n","        optimizer_D1.zero_grad()\n","\n","        loss_D1 = D1.compute_loss(X1, valid) + D1.compute_loss(X21.detach(), fake)\n","\n","        loss_D1.backward()\n","        optimizer_D1.step()\n","\n","        # -----------------------\n","        #  Train Discriminator 2\n","        # -----------------------\n","\n","        optimizer_D2.zero_grad()\n","\n","        loss_D2 = D2.compute_loss(X2, valid) + D2.compute_loss(X12.detach(), fake)\n","\n","        loss_D2.backward()\n","        optimizer_D2.step()\n","\n","        # --------------\n","        #  Log Progress\n","        # --------------\n","\n","        # Determine approximate time left\n","        batches_done = epoch * len(dataloader) + i\n","        batches_left = opt.n_epochs * len(dataloader) - batches_done\n","        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n","        prev_time = time.time()\n","\n","        # Print log\n","        sys.stdout.write(\n","            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] ETA: %s\"\n","            % (epoch, opt.n_epochs, i, len(dataloader), (loss_D1 + loss_D2).item(), loss_G.item(), time_left)\n","        )\n","\n","        # If at sample interval save image\n","        if batches_done % opt.sample_interval == 0:\n","            sample_images(batches_done)\n","\n","    # Update learning rates\n","    lr_scheduler_G.step()\n","    lr_scheduler_D1.step()\n","    lr_scheduler_D2.step()\n","\n","    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n","        # Save model checkpoints\n","        torch.save(Enc1.state_dict(), \"saved_models/%s/Enc1_%d.pth\" % (opt.dataset_name, epoch))\n","        torch.save(Dec1.state_dict(), \"saved_models/%s/Dec1_%d.pth\" % (opt.dataset_name, epoch))\n","        torch.save(Enc2.state_dict(), \"saved_models/%s/Enc2_%d.pth\" % (opt.dataset_name, epoch))\n","        torch.save(Dec2.state_dict(), \"saved_models/%s/Dec2_%d.pth\" % (opt.dataset_name, epoch))\n","        torch.save(D1.state_dict(), \"saved_models/%s/D1_%d.pth\" % (opt.dataset_name, epoch))\n","        torch.save(D2.state_dict(), \"saved_models/%s/D2_%d.pth\" % (opt.dataset_name, epoch))"]},{"cell_type":"code","source":["import pandas as pd\n","df=pd.DataFrame({\"loss_GAN_1\":lG1,\"loss_GAN_2\":lG2,\"loss_ID_1\":lI1,\"loss_ID_2\":lI2,\"loss_s_1\":lS1,\"loss_s_2\":lS2,\"loss_c_1\":lC1,\"loss_c_2\":lC2,\n","                 \"loss_Cy_1\":lCy1,\"loss_Cy_2\":lCy2,\"TLoss\":totL})\n","df.to_csv(\"loss.csv\",index=False)\n","fig=df.plot(y=\"TLoss\", color='red').get_figure()\n","fig.savefig('Map_lossplot.jpg')"],"metadata":{"id":"0f6I_43rVFFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Video"],"metadata":{"id":"XqXbeRbd_rn_"}},{"cell_type":"code","source":["import cv2 as cv\n","vid_file=\"story.mp4\"#!ls *.mp4\n","folder=vid_file.split('.')[0]\n","!mkdir $folder\n","totalvidframes = 0\n","vid = cv.VideoCapture(vid_file)\n","vid_writer = cv.VideoWriter(\"gen_\"+vid_file, cv.VideoWriter_fourcc(*'mp4v'), vid.get(cv.CAP_PROP_FPS), (round(vid.get(cv.CAP_PROP_FRAME_WIDTH)),round(vid.get(cv.CAP_PROP_FRAME_HEIGHT))))\n","while (True):\n","    suc, vfr = vid.read()\n","    if not suc:\n","        break\n","    cv.imwrite(f'{folder}/frame_{totalvidframes}.jpg', vfr)\n","    totalvidframes+=1\n","vid.release()"],"metadata":{"id":"d7vclC_V5Zr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(totalvidframes):\n","    cap=cv.VideoCapture(folder+\"/frame_\"+str(i)+\".jpg\")\n","    ret, frame=cap.read()\n","    vid_writer.write(frame)\n","    cap.release()\n","vid_writer.release()"],"metadata":{"id":"j9Dq_V-y8KJy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnpPFd4BYG+Z/QhrOgrGgE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}